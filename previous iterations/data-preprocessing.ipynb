{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d72c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import kagglehub\\n\\n# Download latest version\\npath = kagglehub.dataset_download(\"galaxyh/kdd-cup-1999-data\")\\n\\nprint(\"Path to dataset files:\", path)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"galaxyh/kdd-cup-1999-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a132a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\k'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\k'\n",
      "C:\\Users\\OWNER'S\\AppData\\Local\\Temp\\ipykernel_4468\\3180835246.py:20: SyntaxWarning: invalid escape sequence '\\k'\n",
      "  df = pd.read_csv('dataset\\kddcup.data\\kddcup.data', names=column_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4898431, 42)\n",
      "Sample rows:\n",
      "    duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
      "0         0           tcp    http   SF        215      45076     0   \n",
      "1         0           tcp    http   SF        162       4528     0   \n",
      "2         0           tcp    http   SF        236       1228     0   \n",
      "3         0           tcp    http   SF        233       2032     0   \n",
      "4         0           tcp    http   SF        239        486     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                   0   \n",
      "1               0       0    0  ...                   1   \n",
      "2               0       0    0  ...                   2   \n",
      "3               0       0    0  ...                   3   \n",
      "4               0       0    0  ...                   4   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                     0.0                     0.0   \n",
      "1                     1.0                     0.0   \n",
      "2                     1.0                     0.0   \n",
      "3                     1.0                     0.0   \n",
      "4                     1.0                     0.0   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.00                          0.0   \n",
      "1                         1.00                          0.0   \n",
      "2                         0.50                          0.0   \n",
      "3                         0.33                          0.0   \n",
      "4                         0.25                          0.0   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                   0.0                       0.0                   0.0   \n",
      "1                   0.0                       0.0                   0.0   \n",
      "2                   0.0                       0.0                   0.0   \n",
      "3                   0.0                       0.0                   0.0   \n",
      "4                   0.0                       0.0                   0.0   \n",
      "\n",
      "   dst_host_srv_rerror_rate    label  \n",
      "0                       0.0  normal.  \n",
      "1                       0.0  normal.  \n",
      "2                       0.0  normal.  \n",
      "3                       0.0  normal.  \n",
      "4                       0.0  normal.  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Label distribution:\n",
      " label\n",
      "smurf.              2807886\n",
      "neptune.            1072017\n",
      "normal.              972781\n",
      "satan.                15892\n",
      "ipsweep.              12481\n",
      "portsweep.            10413\n",
      "nmap.                  2316\n",
      "back.                  2203\n",
      "warezclient.           1020\n",
      "teardrop.               979\n",
      "pod.                    264\n",
      "guess_passwd.            53\n",
      "buffer_overflow.         30\n",
      "land.                    21\n",
      "warezmaster.             20\n",
      "imap.                    12\n",
      "rootkit.                 10\n",
      "loadmodule.               9\n",
      "ftp_write.                8\n",
      "multihop.                 7\n",
      "phf.                      4\n",
      "perl.                     3\n",
      "spy.                      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Defining column names\n",
    "column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
    "    'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "    'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
    "    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
    "    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "    'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "    'dst_host_srv_rerror_rate', 'label'\n",
    "]\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('dataset\\kddcup.data\\kddcup.data', names=column_names)\n",
    "\n",
    "# explorating dataset\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Sample rows:\\n\", df.head())\n",
    "print(\"Label distribution:\\n\", df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22cdb54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column data types:\n",
      " duration                         int64\n",
      "protocol_type                   object\n",
      "service                         object\n",
      "flag                            object\n",
      "src_bytes                        int64\n",
      "dst_bytes                        int64\n",
      "land                             int64\n",
      "wrong_fragment                   int64\n",
      "urgent                           int64\n",
      "hot                              int64\n",
      "num_failed_logins                int64\n",
      "logged_in                        int64\n",
      "num_compromised                  int64\n",
      "root_shell                       int64\n",
      "su_attempted                     int64\n",
      "num_root                         int64\n",
      "num_file_creations               int64\n",
      "num_shells                       int64\n",
      "num_access_files                 int64\n",
      "num_outbound_cmds                int64\n",
      "is_host_login                    int64\n",
      "is_guest_login                   int64\n",
      "count                            int64\n",
      "srv_count                        int64\n",
      "serror_rate                    float64\n",
      "srv_serror_rate                float64\n",
      "rerror_rate                    float64\n",
      "srv_rerror_rate                float64\n",
      "same_srv_rate                  float64\n",
      "diff_srv_rate                  float64\n",
      "srv_diff_host_rate             float64\n",
      "dst_host_count                   int64\n",
      "dst_host_srv_count               int64\n",
      "dst_host_same_srv_rate         float64\n",
      "dst_host_diff_srv_rate         float64\n",
      "dst_host_same_src_port_rate    float64\n",
      "dst_host_srv_diff_host_rate    float64\n",
      "dst_host_serror_rate           float64\n",
      "dst_host_srv_serror_rate       float64\n",
      "dst_host_rerror_rate           float64\n",
      "dst_host_srv_rerror_rate       float64\n",
      "label                           object\n",
      "dtype: object\n",
      "\n",
      "Unique protocol types: ['tcp' 'udp' 'icmp']\n",
      "Unique service types (first 10): ['http' 'smtp' 'domain_u' 'auth' 'finger' 'telnet' 'eco_i' 'ftp' 'ntp_u'\n",
      " 'ecr_i']\n",
      "Unique flags: ['SF' 'S2' 'S1' 'S3' 'OTH' 'REJ' 'RSTO' 'S0' 'RSTR' 'RSTOS0' 'SH']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nColumn data types:\\n\", df.dtypes)\n",
    "print(\"\\nUnique protocol types:\", df['protocol_type'].unique())\n",
    "print(\"Unique service types (first 10):\", df['service'].unique()[:10])\n",
    "print(\"Unique flags:\", df['flag'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3617354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution:\n",
      " label\n",
      "smurf.              2807886\n",
      "neptune.            1072017\n",
      "normal.              972781\n",
      "satan.                15892\n",
      "ipsweep.              12481\n",
      "portsweep.            10413\n",
      "nmap.                  2316\n",
      "back.                  2203\n",
      "warezclient.           1020\n",
      "teardrop.               979\n",
      "pod.                    264\n",
      "guess_passwd.            53\n",
      "buffer_overflow.         30\n",
      "land.                    21\n",
      "warezmaster.             20\n",
      "imap.                    12\n",
      "rootkit.                 10\n",
      "loadmodule.               9\n",
      "ftp_write.                8\n",
      "multihop.                 7\n",
      "phf.                      4\n",
      "perl.                     3\n",
      "spy.                      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack vs Normal:\n",
      " is_attack\n",
      "1    3925650\n",
      "0     972781\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 attack types:\n",
      " label\n",
      "smurf.          2807886\n",
      "neptune.        1072017\n",
      "satan.            15892\n",
      "ipsweep.          12481\n",
      "portsweep.        10413\n",
      "nmap.              2316\n",
      "back.              2203\n",
      "warezclient.       1020\n",
      "teardrop.           979\n",
      "pod.                264\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count label frequencies\n",
    "print(\"\\nLabel distribution:\\n\", df['label'].value_counts())\n",
    "\n",
    "# Check how many are normal vs attack\n",
    "df['is_attack'] = df['label'].apply(lambda x: 0 if x == 'normal.' else 1)\n",
    "print(\"\\nAttack vs Normal:\\n\", df['is_attack'].value_counts())\n",
    "\n",
    "# (Optional) View top 10 attack types by frequency\n",
    "attack_counts = df[df['label'] != 'normal.']['label'].value_counts()\n",
    "print(\"\\nTop 10 attack types:\\n\", attack_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e759b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attack category distribution:\n",
      " attack_category\n",
      "dos       3883370\n",
      "normal     972781\n",
      "probe       41102\n",
      "r2l          1126\n",
      "u2r            52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load attack type mapping (if available)\n",
    "attack_map = pd.read_csv('dataset/training_attack_types', sep=' ', names=['attack_type', 'category'], index_col=0)\n",
    "\n",
    "# Map labels to broader categories\n",
    "df['attack_category'] = df['label'].apply(lambda x: 'normal' if x == 'normal.' else attack_map.loc[x[:-1], 'category'])\n",
    "print(\"\\nAttack category distribution:\\n\", df['attack_category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6a2cf",
   "metadata": {},
   "source": [
    "##### Shape: 4,898,431 rows × 42 columns.\n",
    "##### Identified 3 categorical features (protocol_type, service, flag) and 38 numerical ones.\n",
    "##### Explored label distribution:    normal.: ~972K rows    Attacks: ~3.93M rows (dominant: smurf., neptune.)\n",
    "##### Added a binary is_attack flag for future evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e18963",
   "metadata": {},
   "source": [
    "## Step 3 Preprocessing for Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ecadcdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4468\\317569090.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m X_cat = encoder.fit_transform(X_raw[categorical_cols])\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Standardize numerical features\u001b[39;00m\n\u001b[32m     20\u001b[39m scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m X_num = scaler.fit_transform(X_raw[numerical_cols])\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Combine processed features\u001b[39;00m\n\u001b[32m     24\u001b[39m X_processed = np.hstack((X_num, X_cat))\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    888\u001b[39m                 )\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    891\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    893\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    894\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    895\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    903\u001b[39m             Fitted scaler.\n\u001b[32m    904\u001b[39m         \"\"\"\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1359\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m                 )\n\u001b[32m   1362\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    939\u001b[39m         self : object\n\u001b[32m    940\u001b[39m             Fitted scaler.\n\u001b[32m    941\u001b[39m         \"\"\"\n\u001b[32m    942\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m         X = validate_data(\n\u001b[32m    944\u001b[39m             self,\n\u001b[32m    945\u001b[39m             X,\n\u001b[32m    946\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2950\u001b[39m             out = y\n\u001b[32m   2951\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2952\u001b[39m             out = X, y\n\u001b[32m   2953\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2955\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2957\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1050\u001b[39m                         )\n\u001b[32m   1051\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1053\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m                 raise ValueError(\n\u001b[32m   1056\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    754\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    755\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    758\u001b[39m \n\u001b[32m    759\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32md:\\softwares\\Dev\\Python\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2164\u001b[39m             )\n\u001b[32m   2165\u001b[39m         values = self._values\n\u001b[32m   2166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2171\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'normal'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Drop constant column\n",
    "df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
    "\n",
    "# Ensure 'is_attack' doesn't leak into feature matrix\n",
    "X_raw = df.drop(columns=['label', 'is_attack']) if 'is_attack' in df.columns else df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "numerical_cols = X_raw.columns.difference(categorical_cols)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_cat = encoder.fit_transform(X_raw[categorical_cols])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(X_raw[numerical_cols])\n",
    "\n",
    "# Combine processed features\n",
    "X_processed = np.hstack((X_num, X_cat))\n",
    "\n",
    "print(\"Final processed feature shape:\", X_processed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f160e",
   "metadata": {},
   "source": [
    "##### Dropping num_outbound_cmds because it has the same value (i.e. 0) for all rows and carries no useful information for learning.\n",
    "##### Column isattack removed for training\n",
    "##### Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734dfccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"dataset/X_processed.npy\", X_processed)\n",
    "np.save(\"dataset/y_true.npy\", df['is_attack'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cc890",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m joblib.dump(\u001b[43mencoder\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mencoder.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m joblib.dump(scaler, \u001b[33m\"\u001b[39m\u001b[33mscaler.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(encoder, \"encoder.joblib\")\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
